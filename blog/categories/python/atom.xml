<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | 刘毅同学]]></title>
  <link href="http://liuyix.org/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://liuyix.org/"/>
  <updated>2017-03-07T00:43:59+08:00</updated>
  <id>http://liuyix.org/</id>
  <author>
    <name><![CDATA[liuyix]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python Yield小结]]></title>
    <link href="http://liuyix.org/blog/2016/python-yield/"/>
    <updated>2016-07-10T21:51:00+08:00</updated>
    <id>http://liuyix.org/blog/2016/python-yield</id>
    <content type="html"><![CDATA[<p><code>yield</code> 是用来简化以下场景：函数来生成序列，并且使用遍历的方式来访问序列中的元素。</p>

<p><code>yield</code>的实现原理理解上来说在调用<code>yield</code>时Python会保留函数的<em>现场</em>，当再次遍历时函数的状态不丢失，可以继续生成。</p>

<h3>经典的例子斐波那契数列</h3>

<h4>问题描述</h4>

<p>返回斐波那契数列前n个元素</p>

<h4>Python解法</h4>

<h5>第一个版本：朴素实现</h5>

<p>```python
def fab(n):</p>

<pre><code>fab_list = list()
i = 0
a, b = 0, 1
while i &lt; n:
    a, b = b, a+b
    fab_list.append(a)
    i = i + 1
return fab_list  
</code></pre>

<p>```</p>

<p>第一个版本是遍历并保存所有前n项斐波那契数列的元素。最大的问题是会占用非常多的内存，当调用fab(10000)时，在我的电脑中已经是无法完成的了。</p>

<h5>第二个版本: 简单的迭代器实现</h5>

<p>实现一个迭代器类</p>

<p>```python
class Fab(object):</p>

<pre><code>def __init__(self, n):
    self.n = n
    self.a, self.b = 0, 1
    self.i = 0

def __iter__(self):
    return self

def next(self):
    if self.i &lt; self.n:
        r = self.b
        self.a, self.b = self.b, self.a + self.b
        self.i = self.i + 1
        return r
    raise StopIteration()
</code></pre>

<p>if <strong>name</strong> == &lsquo;<strong>main</strong>&rsquo;:</p>

<pre><code>for i in Fab(5):
    print i       
</code></pre>

<p>```</p>

<p>第二个版本实现了迭代器，每次调用时再生成下一个元素，因此对内存的占用是恒定值。但缺点是代码很长，不够易读。</p>

<h5>第三个版本：yield方法</h5>

<p>```python
def fab(n):</p>

<pre><code>i, a, b = 0, 0, 1
while i &lt; n:
    a, b = b, a+b
    yield a
    i = i + 1
</code></pre>

<p>if <strong>name</strong> == &lsquo;<strong>main</strong>&rsquo;:</p>

<pre><code>for n in fab(5):
    print n
print fab(5)
</code></pre>

<p>```</p>

<p>第三种yield方法兼具了第一种的简洁，第二种的高效。</p>

<p><code>fab(5)</code>返回的是一个<a href="https://wiki.python.org/moin/Generators"><code>generator</code></a>对象——让一个函数像iterator那样工作，这样在遍历的场景下既可以保持代码简洁，又保持了内存使用的高效。</p>

<h4>相关介绍链接</h4>

<ol>
<li><a href="http://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained">Improve Your Python: &lsquo;yield&rsquo; and Generators Explained</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/">Python yield 使用浅析</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python流式压缩和解压缩调研总结]]></title>
    <link href="http://liuyix.org/blog/2015/python-compress-stream/"/>
    <updated>2015-12-26T22:52:00+08:00</updated>
    <id>http://liuyix.org/blog/2015/python-compress-stream</id>
    <content type="html"><![CDATA[<h3>Changelog</h3>

<table>
<thead>
<tr>
<th></th>
<th>  Time </th>
<th> Description </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> 2016-01-05 </td>
<td> 小幅修改</td>
</tr>
<tr>
<td></td>
<td> 2015-12-26 </td>
<td> initial version</td>
</tr>
</tbody>
</table>


<p>最近两周在研究和实现了xtrabackup流式压缩上传&amp;下方案，这里做一下总结分享。</p>

<p>实现的效果是备份不在本地暂存，压缩内置到上传逻辑中，通过配置文件可以配置压缩细节，对上传使用者来讲是『无感知』的。利用压缩提高了上传效率和备份空间的使用率。</p>

<h2>压缩率和压缩性能的比较</h2>

<ul>
<li>lz4</li>
<li>gzip/pigz</li>
<li>lzop</li>
<li>qpress</li>
</ul>


<p>结论：在兼顾压缩率和压缩占用CPU资源以及压缩效率几个方面，最终选择了lzop。</p>

<p>lz4据说是压缩最快的算法，最终没有选择，有几个方面的原因：</p>

<ul>
<li>Python支持不完善，因为我的需求是<strong>流式</strong>的上传和下载，压缩源是一个tar stream，和压缩后的文件不会落在本地，而是直接上传到远端。</li>
<li>可运维性不高：lzo文件没有找到和gzip, lzop这样的命令行工具可以一个命令来对一个完整的lz4文件管理。当调用<code>file a.lz4</code> 显示的文件是data，这样就无法确认这个文件到底是否正确了。</li>
</ul>


<p>gzip给力的地方：</p>

<ul>
<li>gzip是目前应用最为广泛的格式了，看过很多压缩率评测的文章，gzip压缩率十分给力，在compresslevel是7到9时，压缩比很给力。</li>
<li>Python也是在standard library中支持gzip的压缩和解压缩。</li>
</ul>


<p>gzip不够给力的地方：</p>

<ul>
<li>压缩速度慢，在高压缩率上很慢。但是compress level在2~3时，gzip无论是压缩比还是压缩效率上都很有竞争优势</li>
<li>Python标准库gzip压缩对stream支持不够，流式压缩期望的方法是：传入一个fileobj，返回一个fileobj。而Gzip模块直接用的话是不支持这种方式的。</li>
</ul>


<p>然而gzip不足点是可以弥补的。
首先是压缩速度上，默认gzip是不支持并行化的压缩，最多只能有单核的性能，这是压缩性能的瓶颈。但是gzip是有其他工具支持<strong>并行压缩的</strong> —— <code>pigz</code>就是并行压缩版本的gzip，测试结果显示pigz可以充分利用多核并行化的性能，让压缩时间有明显的减少，当然代价就是load也会成倍的增长（24核机器上，我只测试了2~8个线程数）</p>

<p>其次是流式压缩可以通过Python轻松实现数据分块并行压缩。这里很重要的一点是：<strong>可以将每个数据分块看做一个独立文件压缩最终可以合成一个符合Gzip格式的压缩文件</strong>
简单的看了下的pigz的代码注释，发现pigz的原理也是这样子的。Python中使用生产者和消费者模式（生产者：一个线程专门来从数据源拉数据，数据存入Queue中，多个压缩进程负责压缩，把压缩后数据再放入到PriorityQueue按顺序组合成一个文件即可，当然也可以直接分块上传到远端服务器）。这也是我实现的第一个版本的流式压缩方案。这个方案主要的弱点在于内存资源占用比较大：无论是从数据源读取的原始数据块还是压缩后的压缩数据块，在最终写入/上传前都需要缓存在内存中。
在流式压缩上传的场景下，压缩上传的输出端是REST上传接口，因此并行压缩的内存占用上一定程度上受上传接口的性能影响。极端的情况，当上传速度很慢时，为了不影响压缩效率则需要在内存中开辟更大的内存buffer来放入等待压缩的数据块，这时就会占用到比较大的内存资源。不过也可以通过限制上传队列大小，在队列满的情况，数据源的<code>write</code>会阻塞等待。</p>

<p>我的第二个方案的是利用pigz外部工具在读取数据源前利用管道先接入到<code>pigz</code>，在从<code>pigz</code>直接读取到压缩后的数据流。这个方案算是最终方案的替代方案，之所以没有最终使用，原因在于和lzop相比，在压缩率相近的情况下，<code>pigz</code>消耗了更多的CPU资源。</p>

<p>对比方案：<code>pigz -p 2 -2</code> vs <code>lzop -c</code> ，即通过2线程压缩等级2和默认的lzop对比，前者压缩比可以高出10%，但是Load比后者高出了2倍不止，同时速度上也慢了25%。</p>

<p>lzop给力的地方：
实际测试中，lzop兼顾性能和压缩比同时压缩占用的CPU上是最平衡的。压缩速度稍慢于lz4，压缩比上可以达到gzip等级2~3的水平，同时cpu占用率和内存上比gzip低。</p>

<p>lzop不给力的地方：</p>

<ul>
<li>Python bindings 接口较少，不能直接用来流式压缩上传。

<ul>
<li>通过 <em>外挂</em> 方式也可以完成。（其实即使支持的好，也需要多进程来并行工作，和外挂差别不大）</li>
</ul>
</li>
</ul>


<h2>总结</h2>

<ul>
<li>lzop和pigz这两个压缩方案上在实际应用上都很有竞争力，只是我涉及的项目需求是生产环境上尽量不占用过多CPU，MEM资源，可以在压缩比上做妥协。因此选择了lzop。如果你在意CPU资源，更在乎压缩数据大小，则pigz是不错的选择。</li>
<li><em>外挂</em> 方式的数据压缩方案虽然集成上让Python程序有更多的外部依赖，但是考虑到Python本身并没有真正的线程的并行方案（你想要并行也要生成多个进程），其实资源占用区别不大。同时方案选型上也更加灵活。</li>
</ul>


<table>
<thead>
<tr>
<th></th>
<th> 压缩工具 </th>
<th> 描述 </th>
<th> 参数 </th>
<th> 优势 </th>
<th> 劣势 </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> gzip </td>
<td> 最常用的压缩工具 </td>
<td> <code>-2</code> </td>
<td> 压缩比在压缩参数大于2时很高，平台上通用很高，所有Linux发行版都会有预装，同时tar也集成了gzip压缩 </td>
<td> 单线程，较慢</td>
</tr>
<tr>
<td></td>
<td> pigz </td>
<td> 多线程版gzip，<a href="http://zlib.net/pigz/">主页</a>, <a href="https://github.com/madler/pigz">github</a> </td>
<td> 多线程并行+gzip压缩算法，无论从性能上还是压缩输出上都很不错，是个很不错的选择 </td>
<td> 压缩效率和系统资源占用成正比</td>
<td></td>
</tr>
<tr>
<td></td>
<td> lzop </td>
<td> </td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python中的subprocess与Pipe]]></title>
    <link href="http://liuyix.org/blog/2015/python-subprocess-pitfalls/"/>
    <updated>2015-12-23T21:31:00+08:00</updated>
    <id>http://liuyix.org/blog/2015/python-subprocess-pitfalls</id>
    <content type="html"><![CDATA[<h2>一、纠结的困境</h2>

<p>Python在多进程上不是很令人满意，尤其是<code>subprocess</code>模块。当用Python实现一个shell脚本中的管道时就出现了比较尴尬的局面。</p>

<h3>大数据量的管道问题</h3>

<p>subprocess模块有两种方式来和生成的子进程交互：<code>wait</code>和<code>communicate</code>。
关于<code>wait</code>文档中有以下说明：</p>

<blockquote><p>Warning This will deadlock when using stdout=PIPE and/or stderr=PIPE and the child process generates enough output to a pipe such that it blocks waiting for the OS pipe buffer to accept more data. Use communicate() to avoid that.</p></blockquote>

<p>即当stdout/stdin设置为PIPE时，使用<code>wait()</code>可能会导致死锁。因而建议使用<code>communicate</code></p>

<p>而对于<code>communicate</code>，文档又给出：</p>

<blockquote><p>Note The data read is buffered in memory, so do not use this method if the data size is large or unlimited.</p></blockquote>

<p><code>communicate</code>会把数据读入内存缓存下来，所以当数据很大或者是无限的数据时不要使用。</p>

<p>那么问题来了：当你要使用Python的<code>subprocess.Popen</code>实现命令行之间的管道传输，同时数据源又非常大（比如读取上GB的文本或者无尽的网络流）时，官方文档不建议用<code>wait</code>，同时<code>communicate</code>还可能把内存撑爆&hellip; <img src="/images/1.jpg" alt="/images/1.jpg" /></p>

<h2>探究实现</h2>

<p>Python在系统编程领域可以理解为就是C语言的一种简写，因为无论是用C语言还是Python都是对系统Linux/Windows API的使用，只是相比于C，Python封装后由于是解释型语言而变得易于编写和调试。</p>

<p>在Python中subprocess是用来创建新的进程，同时创建管道连接子进程的stdout, stderr, stdin（可选）。然后执行子进程，最后得到子进程的return code。</p>

<blockquote><p>This module intends to replace several older modules and functions:
os.system
os.spawn<em>
os.popen</em>
popen2.<em>
commands.</em></p></blockquote>

<p>所以可以理解为Python意图用subprocess模块来统一之前若干生成子进程的方式。</p>

<p>关于subprocess的基础用法再次暂时略过，网上相关的内容很多，有时间再赘述。这里重点说一下shell多管道流对应的python实现。</p>

<h3>一个shell多管道脚本的改写</h3>

<p>shell编程领域，将cli工具结合强大的pipe，可以一行代码就能完成相对复杂的工作，尤其是在文本编辑上。如以下的例子：</p>

<p><code>
ps aux | egrep 'xtrabackup|innobackupex' | grep -v grep | awk '{print $2}' | xargs kill
</code></p>

<p>这是（我比较常用的）杀掉备份进程的一行命令。大致流程如下图所示。</p>

<p><img src="/images/0105-1.svg" alt="流程图" /></p>

<p>那么如果如何将上述代码转换为python脚本表达呢？</p>

<p>推荐的做法：分别生成subprocess子进程，同时用管道相连。</p>

<p><code>
import subprocess
import shlex
ps_proc = subprocess.Popen(shlex.split('ps aux'), stdout=subprocess.PIPE)
grep_proc = subprocess.Popen(shlex.split("egrep 'xtrabackup|innobackupex'"), stdin=ps_proc.stdout, stdout=subprocess.PIPE)
awk_proc = subprocess.Popen(shlex.split('awk "{print $2}"'), stdin=grep_proc.stdout, stdout=subprocess.PIPE)
kill_proc = subprocess.Popen(shlex.split('xargs kill'), stdin=awk_proc.stdout)
</code></p>

<p>每一个子进程的<code>stdin</code>都是上一个子进程的<code>stdout</code>，除最后一个子进程外其余进程的stdout参数都是<code>subprocess.PIPE</code>即管道输出，这样就首先了首尾相连。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python并发实用编程手册[Draft]]]></title>
    <link href="http://liuyix.org/blog/2015/python-concurrent-programming/"/>
    <updated>2015-12-17T07:08:00+08:00</updated>
    <id>http://liuyix.org/blog/2015/python-concurrent-programming</id>
    <content type="html"><![CDATA[<p>Python并发编程<del>常用的</del>builtin就是2个模块：<code>threading</code>和<code>multiprocessing</code>。其中<code>threading</code>因为著名的GIL，实际是<em>伪多线程</em>，每个thread并没有对应一个pthread；<code>multiprocessing</code>则是利用多进程的手段来绕过GIL达到并发的效果。</p>

<p>如果你的任务并非是CPU密集型，而是IO密集型或者网络应用的话——单线程时CPU总是处于等待的状态时，用threading其实影响不大。</p>

<!--more-->


<h2>threading</h2>

<p>Doc: <a href="https://docs.python.org/2/library/threading.html">https://docs.python.org/2/library/threading.html</a></p>

<h3>Thread</h3>

<p>和其他语言的线程创建类似，有两种方式创建线程的方式，一种是创建thread时传入要执行的工作方法，另外一种是创建<code>Thread</code>的子类。</p>

<h4>join</h4>

<p><code>join()</code> 方法提供了不同线程间的协同功能。通常的用法是主线程在创建完所有的工作线程后，循坏调用没有工作线程的<code>join()</code>方法来等待所有线程的结束。<code>join()</code>方法还可以增加<code>timeout</code>参数设置超时时间，可以用来避免一个工作线程出问题使得整个程序死等的情况。</p>

<h4>daemon</h4>

<p><code>daemon</code>是Thread的一个属性，程序退出不关心设置了<code>daemon</code>线程的『死活』。即只要所有非<code>daemon</code>线程都结束了，那么程序就可以结束掉了。
<code>daemon</code>线程类似于程序内部的『后台服务』，提供了程序运行时的『基础设施』，而不是程序工作流程链路中的部分。</p>

<h2>Lock, RLock</h2>

<p>并发编程中，锁是基本原语。当有共享资源需要『互斥』访问时就要用锁来对这些资源进行保护。<del>然而Python语言中由于GIL问题，实际上builtin的数据类型都是thread-safe的，使用时无需上锁</del>对于mutable类型的变量在多线程共享使用时应该用Lock来控制并发，不能依赖Python的具体实现。</p>

<p>在Python 2.6以上，可以使用<code>with</code>简化获取和释放锁的过程，让代码更加简洁。</p>

<h2>Condition</h2>

<p>是对Lock的一层封装。提供了<code>acquire()</code>, <code>release()</code>这两个Lock原语，还提供了<code>wait()</code>, <code>notify()</code>, <code>notifyAll()</code>这三个接口。</p>

<ul>
<li><code>wait()</code> 是获得锁的线程主动释放锁，转入等待过程。</li>
<li><code>notify()</code> 是获得锁的一方不释放锁，但是会唤醒等待这把锁的其中一个线程。</li>
<li><code>notifyAll()</code> 类似<code>notify()</code>，不同的是不是唤醒一个，而是唤醒所有等待锁的线程。</li>
</ul>


<p>典型的使用场景是生产者消费者模式。一个生产者，若干个消费者用一个队列来交互，这时候每个工作线程要想工作首先都需要尝试获得队列的锁。当一个消费者拿到了锁可以访问队列时发现队列时空的，这时候就要使用<code>wait()</code>来主动释放这把锁，然后将自己放到等待线程列表中等待；当一个生产者获得了队列的锁时，将数据放入到队列中，同时在释放锁之前调用<code>notify()</code>来通知其他等待这把锁的线程：『达令，有你快递儿~』，然后在释放锁。如果生产了多个数据，那么就可以使用<code>notifyAll()</code>通知所有的等待线程。</p>

<h2>Event</h2>

<p>Event接口非常简单，就是<code>is_set()</code>和<code>set()</code>, Event用于在多线程间共享某一事件是否发生。我经常用来创建一个<code>stop_event</code>传给所有的worker线程，这样如果程序遇到异常或者其他需要终止的条件时，只要<code>stop_event.set()</code>，其他工作线程会循环的检查<code>stop_event.is_set()</code>。</p>

<h2>Semphore</h2>

<p>上过操作系统课程的同学对『信号量』都有过接触，信号量和对应的<code>p</code>, <code>v</code>操作是并发程序的『操作原语』，即用这两个操作就可以表示所有的并发程序的设计和实现了。上述的Lock, Condition都可以看做是信号量的特定形式。</p>

<p>信号量由一个内部计数器和两个接口构成，内部的计数是非负数，对计数的操作：<code>release()</code>用于增加计数，<code>acquire()</code>用于减计数器。当计数器减到0时，<code>release()</code>操作就会阻塞直至其他工作线程完成<code>acquire()</code>。</p>

<p>Python的<code>acquire()</code>还支持可选参数<code>blocking</code>，当设置为True且计数器为0时，则会阻塞等待。</p>

<blockquote><p>前面的叙述都算是对基本内容的解读，干货现在开始。</p></blockquote>

<h2>如何优雅的中断其他线程的任务</h2>

<h3>使用Event</h3>

<p>在每个线程创建时传入<code>stop_event</code>,遇到异常或者到程序需要结束的时候，则<code>stop_event.set()</code>就可以了。
缺点：只有在线程完成一次工作后才会检查stop_event一次。因此如果线程工作比较久时会设置了<code>stop_event</code>也不会立刻结束。</p>

<p>```python</p>

<h1>worker</h1>

<p>def do_some_job(job_q, stop_event):</p>

<pre><code>while not stop_event.is_set():
    try:
        do_foo()
    except:
        stop_event.set()
</code></pre>

<h1>manager</h1>

<p>import Queue
thread_count = 4
stop_event = threading.Event()
job_queue = Queue.Queue()</p>

<p>threads = [threading.Thread(target=do_some_job, args=(job_queue, stop_event,)) for i in xrange(thread_count)]</p>

<p>for thr in threads:</p>

<pre><code>thr.start()
</code></pre>

<p>for thr in threads:</p>

<pre><code>thr.join()
</code></pre>

<p>```</p>

<h2>Signal的处理</h2>

<p>多线程中Signal只能由<strong>主线程</strong>处理Signal。这部分还没有搞过，暂时略过。</p>

<ul>
<li><a href="https://pymotw.com/2/signal/#signals-and-threads">https://pymotw.com/2/signal/#signals-and-threads</a></li>
<li><a href="http://stackoverflow.com/questions/25676835/signal-handling-in-multi-threaded-python">http://stackoverflow.com/questions/25676835/signal-handling-in-multi-threaded-python</a></li>
<li><a href="https://docs.python.org/2/library/signal.html">https://docs.python.org/2/library/signal.html</a></li>
</ul>


<h2>multiprocessing &ndash; Process-based “threading” interface.</h2>

<p>多进程方式实现threading的接口，从而真正的达到并发。对于计算密集型的应用来说，利用<code>multiprocessing</code>模块可以用Python释放<em>真¤多核</em>性能。</p>

<p>多线程的弊端：多进程相比单进程会有独立的内存地址空间，因此无法共享同父进程的资源，进程切换的系统开销会比多线程多。同时进程间的交互、资源共享也比较复杂和『耗能』。</p>

<h2>Python并发常见编程模式 &mdash; 生产者消费者模式</h2>

<ul>
<li>使用<code>multiprocessing.Queue</code>来进行数据交互</li>
<li>使用<code>multiprocessing.Event</code>来交互事件信号</li>
</ul>


<h2>Python并发常见编程模式 &mdash; Pool + map</h2>

<p>TODO</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docopt -- Python必备的命令行接口模块]]></title>
    <link href="http://liuyix.org/blog/2015/python-docopt/"/>
    <updated>2015-10-21T00:02:00+08:00</updated>
    <id>http://liuyix.org/blog/2015/python-docopt</id>
    <content type="html"><![CDATA[<p>docopt很适合经常需要用python写命令行工具的同学使用。</p>

<h2>docopt之前</h2>

<p>工作需要，经常会用大块的代码来定(ren)义(rou)命令行界面的工具。代码经常是如下的样子：</p>

<p>```python</p>

<p>import optparse</p>

<p>parser = optparse.OptionParser()
parser.add_option(&lsquo;&mdash;foo&rsquo;, &lsquo;-f&rsquo;, default=&lsquo;1&rsquo;, type=&lsquo;int&rsquo;)
parser.add_option(&lsquo;&mdash;bar&rsquo;, &lsquo;-b&rsquo;, action=&lsquo;store_true&rsquo;)</p>

<h1>类似以上的代码大概几十行</h1>

<p>opts, args = parser.parse_args()</p>

<p>```</p>

<p>每一个python脚本都需要提供类似的接口。因此每一次都需要写类似的代码。在写过几次后为了保持DRY原则，我将初始化parser封装为一个method放在util部分。可是依旧是逃不过重复的写<code>parser.add_option</code>。不止一次地我考虑干脆自己写个模板类，以后命令行的定义直接以配置文件的形式写出来，然后每次都通过读取这个配置文件自动化的去生成parser。我相信这个问题我一定不是第一个遇到，应该会有已知的模块解决这个laber intensive的工作。</p>

<p>正在这个时候，偶然看到Python weekly发现了docopt</p>

<h2>docopt</h2>

<p>docopt官网地址：<a href="http://docopt.org/">http://docopt.org/</a></p>

<p>docopt的作者有一个30分钟的视频很好的介绍了docopt这个moudule。推荐大家看一下，自备梯子~ <a href="https://youtu.be/pXhcPJK5cMc">https://youtu.be/pXhcPJK5cMc</a></p>

<p>更令懒人们惊喜的是作者还制作了一个js版本的docopt，可以让你在浏览器中把玩docopt： <a href="http://try.docopt.org/">http://try.docopt.org/</a></p>

<p>使用docopt后，代码上会更加Pythonic，具有很高的可读性，命令行接口的定义所见即所得的样式：</p>

<p>```</p>

<h1>docopt example</h1>

<p>mydoc = &ldquo;&rdquo;&ldquo;Naval Fate.</p>

<p>Usage:
  naval_fate.py ship new <name>&hellip;
  naval_fate.py ship <name> move <x> <y> [&mdash;speed=<kn>]
  naval_fate.py ship shoot <x> <y>
  naval_fate.py mine (set|remove) <x> <y> [&mdash;moored|&mdash;drifting]
  naval_fate.py -h | &mdash;help
  naval_fate.py &mdash;version</p>

<p>Options:
  -h &mdash;help     Show this screen.
  &mdash;version     Show version.
  &mdash;speed=<kn>  Speed in knots [default: 10].
  &mdash;moored      Moored (anchored) mine.
  &mdash;drifting    Drifting mine.
&ldquo;&rdquo;"</p>

<p>from docopt import docopt</p>

<p>arguments = docopt(mydoc, version=&lsquo;0.1&rsquo;)</p>

<p>print arguments
```</p>

<p>这样就定义了一个丰富的命令行接口。命令行接口提供了<code>--help</code>和<code>--version</code>两个基础功能。其中<code>--help</code>输出<code>mydoc</code>，<code>--version</code>输出指定的<code>version</code>信息。</p>

<p>接口还提供了2个参数（ship, mine），每种参数还提供了不同的几种参数的组合。其中<code>[...]</code>内是可选参数，<code>(...|...)</code>是互斥参数。</p>
]]></content>
  </entry>
  
</feed>
